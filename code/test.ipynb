{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader\n"
     ]
    }
   ],
   "source": [
    "from dataloader import dataloader, SignalDataset\n",
    "from utils import prepare_data\n",
    "from param import dataset_path, sample_universe_size\n",
    "from torch.utils.data import ConcatDataset\n",
    "from MTL_w_cascade_info import MtlCascadeModel\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch\n",
    "\n",
    "# from torch.nn.utils import weight_norm\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "hp = {\n",
    "    \"n_layers\": 1,\n",
    "    \"sp_hidden_nodes\": 20,\n",
    "    \"n_sp_hidden_lyrs\": 1,\n",
    "    \"mu_hidden_nodes\": 20,\n",
    "    \"n_mu_hidden_lyrs\": 1,\n",
    "    \"smr_hidden_nodes\": 20,\n",
    "    \"n_smr_hidden_lyrs\": 1,\n",
    "    \"n_epochs\": 100,\n",
    "    \"batch_size\": 20,\n",
    "    \"train_ratio\": 0.8,\n",
    "}\n",
    "combined_dataset = torch.load('dataset.pth')\n",
    "print(\"data_loader\")\n",
    "train_loader, test_loader = dataloader(\n",
    "    datasets=combined_dataset,\n",
    "    train_ratio=hp[\"train_ratio\"],\n",
    "    train_batch_size=hp[\"batch_size\"],\n",
    "    test_batch_size=1,\n",
    ")\n",
    "\n",
    "def train(\n",
    "    train_loader,\n",
    "    model,\n",
    "    epoch,\n",
    "    out_dict,\n",
    "    loss_sp_fn,\n",
    "    loss_mu_fn,\n",
    "    loss_smr_fn,\n",
    "    optimizer,\n",
    "):\n",
    "    correct = 0\n",
    "    for data in train_loader:\n",
    "        feature, label = data\n",
    "        y = [out_dict[x] for x in label]\n",
    "        out_sp, out_mu, out_smr = model(feature)\n",
    "\n",
    "        sp_list = [inner_list[0] for inner_list in y]\n",
    "        mu_list = [inner_list[1] for inner_list in y]\n",
    "        smr_list = [inner_list[2:] for inner_list in y]\n",
    "        y_sp = torch.Tensor(sp_list).unsqueeze(1)\n",
    "        y_mu = torch.Tensor(mu_list).unsqueeze(1)\n",
    "        y_smr = torch.Tensor(smr_list).unsqueeze(1)\n",
    "\n",
    "        loss_sp = loss_sp_fn(out_sp, y_sp)\n",
    "        loss_mu = loss_mu_fn(out_mu, y_mu)\n",
    "        loss_smr = loss_smr_fn(out_smr, y_smr)\n",
    "\n",
    "        total_loss = loss_sp + loss_mu + loss_smr\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_y = torch.cat((out_sp, out_mu, out_smr), dim=1)\n",
    "        result = (pred_y > 0.5).float()\n",
    "        target = torch.tensor(y).float()\n",
    "        for i in range(result.size(0)):\n",
    "            if torch.all(torch.eq(result[i], target[i])):\n",
    "                correct += 1\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, Loss_sp: {loss_sp}, Loss_mu: {loss_mu}, Loss_smr: {loss_smr}, Accuracy: {accuracy}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_init\n",
      "start_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateeks/projects/iitj/su/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([20, 1, 2])) that is different to the input size (torch.Size([20, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/prateeks/projects/iitj/su/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 1, 2])) that is different to the input size (torch.Size([8, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss_sp: 0.5497570037841797, Loss_mu: 1.669691562652588, Loss_smr: 0.08134792745113373, Accuracy: 0.22821576763485477\n",
      "Epoch: 2, Loss_sp: 0.12731775641441345, Loss_mu: 0.3549463748931885, Loss_smr: 0.1975173056125641, Accuracy: 0.36751630112625966\n",
      "Epoch: 3, Loss_sp: 0.13728009164333344, Loss_mu: 0.4615171551704407, Loss_smr: 0.19479364156723022, Accuracy: 0.4013040901007706\n",
      "Epoch: 4, Loss_sp: 0.18401402235031128, Loss_mu: 0.39979904890060425, Loss_smr: 0.3128724694252014, Accuracy: 0.41567871962062836\n",
      "Epoch: 5, Loss_sp: 0.07096236944198608, Loss_mu: 0.4647606313228607, Loss_smr: 0.23608353734016418, Accuracy: 0.40915826911677533\n",
      "Epoch: 6, Loss_sp: 0.35188257694244385, Loss_mu: 0.545046865940094, Loss_smr: 0.19359494745731354, Accuracy: 0.43375815056312983\n",
      "Epoch: 7, Loss_sp: 0.19236329197883606, Loss_mu: 0.5008758902549744, Loss_smr: 0.2694648802280426, Accuracy: 0.4350918790752816\n",
      "Epoch: 8, Loss_sp: 1.4852579832077026, Loss_mu: 0.21463799476623535, Loss_smr: 0.15860165655612946, Accuracy: 0.4460580912863071\n",
      "Epoch: 9, Loss_sp: 0.7786109447479248, Loss_mu: 1.4333815574645996, Loss_smr: 0.2365739941596985, Accuracy: 0.44101956135151155\n",
      "Epoch: 10, Loss_sp: 0.45253580808639526, Loss_mu: 0.7057640552520752, Loss_smr: 0.23522034287452698, Accuracy: 0.45583876704208653\n",
      "Epoch: 11, Loss_sp: 0.39727020263671875, Loss_mu: 0.44263607263565063, Loss_smr: 0.23619888722896576, Accuracy: 0.45643153526970953\n",
      "Epoch: 12, Loss_sp: 0.6125422716140747, Loss_mu: 1.2083640098571777, Loss_smr: 0.28254109621047974, Accuracy: 0.46680497925311204\n",
      "Epoch: 13, Loss_sp: 0.08395571261644363, Loss_mu: 1.084690809249878, Loss_smr: 0.14721539616584778, Accuracy: 0.46828689982216953\n",
      "Epoch: 14, Loss_sp: 0.09495381265878677, Loss_mu: 0.25495126843452454, Loss_smr: 0.15970373153686523, Accuracy: 0.4718435091879075\n",
      "Epoch: 15, Loss_sp: 0.0698404386639595, Loss_mu: 0.18575160205364227, Loss_smr: 0.11056943237781525, Accuracy: 0.4788085358624778\n",
      "Epoch: 16, Loss_sp: 0.04318779334425926, Loss_mu: 0.7130489349365234, Loss_smr: 0.15028204023838043, Accuracy: 0.47347362181387076\n",
      "Epoch: 17, Loss_sp: 0.0818684846162796, Loss_mu: 0.27545076608657837, Loss_smr: 0.19290493428707123, Accuracy: 0.48651452282157676\n",
      "Epoch: 18, Loss_sp: 0.1725568026304245, Loss_mu: 0.4179637134075165, Loss_smr: 0.23797300457954407, Accuracy: 0.46799051570835803\n",
      "Epoch: 19, Loss_sp: 0.1919795572757721, Loss_mu: 0.3622596263885498, Loss_smr: 0.1918148696422577, Accuracy: 0.472732661529342\n",
      "Epoch: 20, Loss_sp: 0.07556981593370438, Loss_mu: 0.18951784074306488, Loss_smr: 0.1937735229730606, Accuracy: 0.4791049199762893\n",
      "Epoch: 21, Loss_sp: 0.056652434170246124, Loss_mu: 0.3204583525657654, Loss_smr: 0.23690558969974518, Accuracy: 0.4842916419679905\n",
      "Epoch: 22, Loss_sp: 0.06018221378326416, Loss_mu: 0.09107986092567444, Loss_smr: 0.23660966753959656, Accuracy: 0.4854771784232365\n",
      "Epoch: 23, Loss_sp: 0.06560888886451721, Loss_mu: 0.3257613480091095, Loss_smr: 0.32533442974090576, Accuracy: 0.4842916419679905\n",
      "Epoch: 24, Loss_sp: 0.10355797410011292, Loss_mu: 0.15672357380390167, Loss_smr: 0.23507201671600342, Accuracy: 0.485773562537048\n",
      "Epoch: 25, Loss_sp: 0.49295443296432495, Loss_mu: 0.532962441444397, Loss_smr: 0.23693539202213287, Accuracy: 0.48295791345583877\n",
      "Epoch: 26, Loss_sp: 0.002992207184433937, Loss_mu: 0.35979998111724854, Loss_smr: 0.15272091329097748, Accuracy: 0.4890337877889745\n",
      "Epoch: 27, Loss_sp: 0.9586302638053894, Loss_mu: 0.2806282341480255, Loss_smr: 0.19513411819934845, Accuracy: 0.4884410195613515\n",
      "Epoch: 28, Loss_sp: 0.030592553317546844, Loss_mu: 0.49703091382980347, Loss_smr: 0.23568610846996307, Accuracy: 0.4949614700652045\n",
      "Epoch: 29, Loss_sp: 0.3449230492115021, Loss_mu: 0.3887611925601959, Loss_smr: 0.19349335134029388, Accuracy: 0.4927385892116183\n",
      "Epoch: 30, Loss_sp: 0.044806528836488724, Loss_mu: 0.502322256565094, Loss_smr: 0.23655974864959717, Accuracy: 0.49155305275637223\n",
      "Epoch: 31, Loss_sp: 0.0604616180062294, Loss_mu: 0.2117225080728531, Loss_smr: 0.15220046043395996, Accuracy: 0.5001481920569057\n",
      "Epoch: 32, Loss_sp: 0.027247650548815727, Loss_mu: 0.8172510862350464, Loss_smr: 0.27797266840934753, Accuracy: 0.49688796680497926\n",
      "Epoch: 33, Loss_sp: 0.023280655965209007, Loss_mu: 0.4211570918560028, Loss_smr: 0.193600133061409, Accuracy: 0.5056312981624185\n",
      "Epoch: 34, Loss_sp: 0.0258139930665493, Loss_mu: 0.9352917671203613, Loss_smr: 0.1279202401638031, Accuracy: 0.4979253112033195\n",
      "Epoch: 35, Loss_sp: 0.26903030276298523, Loss_mu: 0.2337425947189331, Loss_smr: 0.27614015340805054, Accuracy: 0.495257854179016\n",
      "Epoch: 36, Loss_sp: 0.1445968896150589, Loss_mu: 0.44997668266296387, Loss_smr: 0.19494003057479858, Accuracy: 0.499407231772377\n",
      "Epoch: 37, Loss_sp: 1.4519846439361572, Loss_mu: 0.6738405227661133, Loss_smr: 0.19234758615493774, Accuracy: 0.5105216360403082\n",
      "Epoch: 38, Loss_sp: 0.10961218178272247, Loss_mu: 0.23489254713058472, Loss_smr: 0.19271647930145264, Accuracy: 0.5026674570243035\n",
      "Epoch: 39, Loss_sp: 0.3239405155181885, Loss_mu: 0.22391264140605927, Loss_smr: 0.19444876909255981, Accuracy: 0.5072614107883817\n",
      "Epoch: 40, Loss_sp: 0.02873827889561653, Loss_mu: 0.21354258060455322, Loss_smr: 0.19468092918395996, Accuracy: 0.5026674570243035\n",
      "Epoch: 41, Loss_sp: 0.6267442107200623, Loss_mu: 0.3553236424922943, Loss_smr: 0.3155795633792877, Accuracy: 0.502371072910492\n",
      "Epoch: 42, Loss_sp: 0.012759498320519924, Loss_mu: 0.4984821081161499, Loss_smr: 0.32118529081344604, Accuracy: 0.5038529934795495\n",
      "Epoch: 43, Loss_sp: 1.040339469909668, Loss_mu: 0.19904229044914246, Loss_smr: 0.19458116590976715, Accuracy: 0.5084469472436277\n",
      "Epoch: 44, Loss_sp: 0.22465424239635468, Loss_mu: 0.12108613550662994, Loss_smr: 0.23700019717216492, Accuracy: 0.5069650266745702\n",
      "Epoch: 45, Loss_sp: 0.8207434415817261, Loss_mu: 1.0182462930679321, Loss_smr: 0.28170499205589294, Accuracy: 0.5139300533491404\n",
      "Epoch: 46, Loss_sp: 0.4981269836425781, Loss_mu: 0.16811414062976837, Loss_smr: 0.15640757977962494, Accuracy: 0.505334914048607\n",
      "Epoch: 47, Loss_sp: 0.06560654938220978, Loss_mu: 0.2987141013145447, Loss_smr: 0.1934555321931839, Accuracy: 0.5044457617071725\n",
      "Epoch: 48, Loss_sp: 0.07964906841516495, Loss_mu: 0.8190520405769348, Loss_smr: 0.321718692779541, Accuracy: 0.509484291641968\n",
      "Epoch: 49, Loss_sp: 0.20386771857738495, Loss_mu: 0.18632552027702332, Loss_smr: 0.19655002653598785, Accuracy: 0.5120035566093657\n",
      "Epoch: 50, Loss_sp: 0.12630322575569153, Loss_mu: 0.40782609581947327, Loss_smr: 0.27317914366722107, Accuracy: 0.5142264374629519\n",
      "Epoch: 51, Loss_sp: 0.3731536269187927, Loss_mu: 0.1172741949558258, Loss_smr: 0.15052050352096558, Accuracy: 0.5056312981624185\n",
      "Epoch: 52, Loss_sp: 0.006970367394387722, Loss_mu: 0.27511075139045715, Loss_smr: 0.23657916486263275, Accuracy: 0.520154119739182\n",
      "Epoch: 53, Loss_sp: 0.07317178696393967, Loss_mu: 0.2019628882408142, Loss_smr: 0.32655370235443115, Accuracy: 0.5198577356253705\n",
      "Epoch: 54, Loss_sp: 0.1394340693950653, Loss_mu: 0.1649661511182785, Loss_smr: 0.1475846767425537, Accuracy: 0.5139300533491404\n",
      "Epoch: 55, Loss_sp: 0.13180333375930786, Loss_mu: 0.32551008462905884, Loss_smr: 0.15014423429965973, Accuracy: 0.5179312388855958\n",
      "Epoch: 56, Loss_sp: 0.018658988177776337, Loss_mu: 0.06849575787782669, Loss_smr: 0.19265960156917572, Accuracy: 0.5220806164789568\n",
      "Epoch: 57, Loss_sp: 0.17330415546894073, Loss_mu: 0.6385607719421387, Loss_smr: 0.19368472695350647, Accuracy: 0.5143746295198577\n",
      "Epoch: 58, Loss_sp: 1.030474066734314, Loss_mu: 1.5869765281677246, Loss_smr: 0.23665155470371246, Accuracy: 0.5154119739181979\n",
      "Epoch: 59, Loss_sp: 0.012475513853132725, Loss_mu: 0.40040910243988037, Loss_smr: 0.23564356565475464, Accuracy: 0.5222288085358625\n",
      "Epoch: 60, Loss_sp: 0.038223959505558014, Loss_mu: 0.721898078918457, Loss_smr: 0.2355971336364746, Accuracy: 0.5179312388855958\n",
      "Epoch: 61, Loss_sp: 0.023291734978556633, Loss_mu: 0.27517449855804443, Loss_smr: 0.19322684407234192, Accuracy: 0.517190278601067\n",
      "Epoch: 62, Loss_sp: 0.10332901775836945, Loss_mu: 0.1981877237558365, Loss_smr: 0.15111775696277618, Accuracy: 0.5241553052756373\n",
      "Epoch: 63, Loss_sp: 0.011696649715304375, Loss_mu: 0.08753830194473267, Loss_smr: 0.28251001238822937, Accuracy: 0.5205986959098993\n",
      "Epoch: 64, Loss_sp: 0.029795484617352486, Loss_mu: 0.30212002992630005, Loss_smr: 0.15108223259449005, Accuracy: 0.5294902193242442\n",
      "Epoch: 65, Loss_sp: 0.04379352927207947, Loss_mu: 0.41464585065841675, Loss_smr: 0.2782686948776245, Accuracy: 0.5263781861292235\n",
      "Epoch: 66, Loss_sp: 0.08812466263771057, Loss_mu: 0.9259266257286072, Loss_smr: 0.19288812577724457, Accuracy: 0.521339656194428\n",
      "Epoch: 67, Loss_sp: 0.11617401242256165, Loss_mu: 0.6369978189468384, Loss_smr: 0.14610238373279572, Accuracy: 0.522525192649674\n",
      "Epoch: 68, Loss_sp: 0.10428404808044434, Loss_mu: 0.2647008001804352, Loss_smr: 0.278400182723999, Accuracy: 0.532602252519265\n",
      "Epoch: 69, Loss_sp: 0.04438622295856476, Loss_mu: 0.1067514419555664, Loss_smr: 0.15316957235336304, Accuracy: 0.52371072910492\n",
      "Epoch: 70, Loss_sp: 0.37967032194137573, Loss_mu: 0.258920818567276, Loss_smr: 0.14825144410133362, Accuracy: 0.5259336099585062\n",
      "Epoch: 71, Loss_sp: 0.0053537385538220406, Loss_mu: 0.2427351474761963, Loss_smr: 0.23565344512462616, Accuracy: 0.5291938352104327\n",
      "Epoch: 72, Loss_sp: 0.11506857722997665, Loss_mu: 0.06852470338344574, Loss_smr: 0.1929154396057129, Accuracy: 0.524896265560166\n",
      "Epoch: 73, Loss_sp: 0.3699191212654114, Loss_mu: 0.5063562989234924, Loss_smr: 0.19478359818458557, Accuracy: 0.5277119146413752\n",
      "Epoch: 74, Loss_sp: 0.4993245303630829, Loss_mu: 0.29050329327583313, Loss_smr: 0.1986207365989685, Accuracy: 0.530823947836396\n",
      "Epoch: 75, Loss_sp: 0.004065620247274637, Loss_mu: 0.34904834628105164, Loss_smr: 0.32324326038360596, Accuracy: 0.5234143449911085\n",
      "Epoch: 76, Loss_sp: 0.3116166293621063, Loss_mu: 0.20629259943962097, Loss_smr: 0.23563340306282043, Accuracy: 0.523117960877297\n",
      "Epoch: 77, Loss_sp: 0.12486329674720764, Loss_mu: 0.5010483264923096, Loss_smr: 0.27483513951301575, Accuracy: 0.529045643153527\n",
      "Epoch: 78, Loss_sp: 0.11433912068605423, Loss_mu: 0.09895466268062592, Loss_smr: 0.15539902448654175, Accuracy: 0.5268227622999407\n",
      "Epoch: 79, Loss_sp: 0.10199185460805893, Loss_mu: 0.1468074768781662, Loss_smr: 0.23575550317764282, Accuracy: 0.5367516301126259\n",
      "Epoch: 80, Loss_sp: 0.03766275942325592, Loss_mu: 0.4109733998775482, Loss_smr: 0.19370540976524353, Accuracy: 0.5265263781861292\n",
      "Epoch: 81, Loss_sp: 0.05374683439731598, Loss_mu: 0.1002098098397255, Loss_smr: 0.23791912198066711, Accuracy: 0.5312685240071132\n",
      "Epoch: 82, Loss_sp: 0.20935550332069397, Loss_mu: 0.1794988065958023, Loss_smr: 0.15211117267608643, Accuracy: 0.5315649081209247\n",
      "Epoch: 83, Loss_sp: 0.004941090010106564, Loss_mu: 0.4604533314704895, Loss_smr: 0.1959909349679947, Accuracy: 0.533787788974511\n",
      "Epoch: 84, Loss_sp: 0.09931455552577972, Loss_mu: 0.29642435908317566, Loss_smr: 0.2365696132183075, Accuracy: 0.525489033787789\n",
      "Epoch: 85, Loss_sp: 0.10597621649503708, Loss_mu: 0.3471388816833496, Loss_smr: 0.1948537528514862, Accuracy: 0.5354179016004742\n",
      "Epoch: 86, Loss_sp: 0.0018213210860267282, Loss_mu: 0.5845110416412354, Loss_smr: 0.1555405706167221, Accuracy: 0.5299347954949615\n",
      "Epoch: 87, Loss_sp: 1.2867610454559326, Loss_mu: 4.384986877441406, Loss_smr: 0.10557664185762405, Accuracy: 0.5363070539419087\n",
      "Epoch: 88, Loss_sp: 0.026725202798843384, Loss_mu: 0.24620279669761658, Loss_smr: 0.15220080316066742, Accuracy: 0.5380853586247777\n",
      "Epoch: 89, Loss_sp: 0.5156785845756531, Loss_mu: 0.3610740303993225, Loss_smr: 0.237204447388649, Accuracy: 0.5342323651452282\n",
      "Epoch: 90, Loss_sp: 0.055538687855005264, Loss_mu: 0.2691245675086975, Loss_smr: 0.19500266015529633, Accuracy: 0.527267338470658\n",
      "Epoch: 91, Loss_sp: 0.06990907341241837, Loss_mu: 0.5301470756530762, Loss_smr: 0.14756828546524048, Accuracy: 0.5389745109662122\n",
      "Epoch: 92, Loss_sp: 0.6897364854812622, Loss_mu: 0.14115798473358154, Loss_smr: 0.15061543881893158, Accuracy: 0.534380557202134\n",
      "Epoch: 93, Loss_sp: 0.0928473100066185, Loss_mu: 0.08818255364894867, Loss_smr: 0.23512157797813416, Accuracy: 0.5336395969176052\n",
      "Epoch: 94, Loss_sp: 0.05749302729964256, Loss_mu: 0.5303348898887634, Loss_smr: 0.1553577035665512, Accuracy: 0.528452874925904\n",
      "Epoch: 95, Loss_sp: 0.3350624144077301, Loss_mu: 0.14217041432857513, Loss_smr: 0.10553223639726639, Accuracy: 0.5277119146413752\n",
      "Epoch: 96, Loss_sp: 0.04298701882362366, Loss_mu: 0.5321062207221985, Loss_smr: 0.2783926725387573, Accuracy: 0.5367516301126259\n",
      "Epoch: 97, Loss_sp: 0.07485216110944748, Loss_mu: 0.64903324842453, Loss_smr: 0.3297373056411743, Accuracy: 0.5321576763485477\n",
      "Epoch: 98, Loss_sp: 0.030167443677783012, Loss_mu: 0.16145367920398712, Loss_smr: 0.1939563751220703, Accuracy: 0.5380853586247777\n",
      "Epoch: 99, Loss_sp: 1.543431282043457, Loss_mu: 0.8561995029449463, Loss_smr: 0.2362738698720932, Accuracy: 0.536158861885003\n",
      "Epoch: 100, Loss_sp: 0.04680952429771423, Loss_mu: 0.17529796063899994, Loss_smr: 0.19629037380218506, Accuracy: 0.534973325429757\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "print(\"model_init\")\n",
    "model = MtlCascadeModel(hp)\n",
    "# print(model)\n",
    "loss_sp = nn.BCEWithLogitsLoss()\n",
    "loss_mu = nn.BCEWithLogitsLoss()\n",
    "loss_smr = nn.MSELoss()\n",
    "\n",
    "out_dict = {\"speech\": [1, 0, 0, 0], \"music\": [0, 1, 0, 0], \"mixture\": [0, 0, 1, 1]}\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = Adam(model.parameters(), lr=0.002)\n",
    "# optimizer = SGD(model.parameters(), lr=0.002, momentum=0.9)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "print(\"start_training\")\n",
    "for epoch in range(1,hp[\"n_epochs\"]+1):\n",
    "    train(\n",
    "        train_loader, model, epoch, out_dict, loss_sp, loss_mu, loss_smr, optimizer\n",
    "    )\n",
    "# train(\n",
    "#     train_loader, model, hp[\"n_epochs\"], out_dict, loss_sp, loss_mu, loss_smr, optimizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the classical fourier trasformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4018/1843205422.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load('audio_file.wav')\n",
      "/home/hparashar/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio_file.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/librosa/core/audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/librosa/core/audio.py:208\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'audio_file.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load audio file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_file.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Perform HPSS\u001b[39;00m\n\u001b[1;32m      7\u001b[0m harmonic, percussive \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mhpss(y)\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/librosa/core/audio.py:183\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/librosa/util/decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/librosa/core/audio.py:239\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    236\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    242\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/mtech_projects/speech_understanding/Speech-Understanding-Minor/.venv/lib/python3.9/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio_file.wav'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load audio file\n",
    "y, sr = librosa.load('audio_file.wav')\n",
    "\n",
    "# Perform HPSS\n",
    "harmonic, percussive = librosa.effects.hpss(y)\n",
    "\n",
    "# Save the harmonic and percussive components\n",
    "librosa.output.write_wav('harmonic.wav', harmonic, sr)\n",
    "librosa.output.write_wav('percussive.wav', percussive, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute skewness\n",
    "def compute_skewness(matrix):\n",
    "    return np.mean(((matrix - np.mean(matrix, axis=0)) / np.std(matrix, axis=0)) ** 3, axis=0)\n",
    "def hpss_classification (speech,music,mixture,sr):\n",
    "    # Perform HPSS\n",
    "    harmonic_speech, percussive_speech = librosa.effects.hpss(speech)\n",
    "    harmonic_music, percussive_music = librosa.effects.hpss(music)\n",
    "    harmonic_mixture, percussive_mixture = librosa.effects.hpss(mixture)\n",
    "\n",
    "    # Compute skewness vectors\n",
    "    skewness_speech = compute_skewness(harmonic_speech) + compute_skewness(percussive_speech)\n",
    "    skewness_music = compute_skewness(harmonic_music) + compute_skewness(percussive_music)\n",
    "    skewness_mixture = compute_skewness(harmonic_mixture) + compute_skewness(percussive_mixture)\n",
    "\n",
    "    # Concatenate skewness vectors for t-SNE\n",
    "    all_skewness = np.vstack((skewness_speech, skewness_music, skewness_mixture))\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    # tsne = TSNE(n_components=2,perplexity=2, random_state=0)\n",
    "    # reduced_skewness = tsne.fit_transform(all_skewness)\n",
    "    reduced_skewness = all_skewness\n",
    "    # Plot the results\n",
    "    print( reduced_skewness.shape)\n",
    "    plt.scatter(reduced_skewness[:1, 0], [0], label='Speech')\n",
    "    plt.scatter(reduced_skewness[1:2, 0], [0], label='Music')\n",
    "    plt.scatter(reduced_skewness[2:, 0], [0]*len(reduced_skewness[2:, 0]), label='Mixture')\n",
    "    plt.legend()\n",
    "    plt.title('t-SNE Visualization of Class Separability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_data\n",
    "import pandas as pd\n",
    "from param import dataset_path, sample_universe_size\n",
    "data_dir = r\"../data\"\n",
    "dataset_path = f\"{data_dir}/musan\"\n",
    "combination_paths = prepare_data(\"../data/musan\",\"../data/speech_music_combinations.csv\")\n",
    "sampled_df = combination_paths.sample(\n",
    "    frac=sample_universe_size, random_state=42, ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.../data/musan'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\".{dataset_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>music</th>\n",
       "      <th>speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/musan/music/fma-western-art/music-fma-wa-...</td>\n",
       "      <td>data/musan/speech/librivox/speech-librivox-012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/musan/music/rfm/music-rfm-0098.wav</td>\n",
       "      <td>data/musan/speech/librivox/speech-librivox-007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/musan/music/jamendo/music-jamendo-0035.wav</td>\n",
       "      <td>data/musan/speech/us-gov/speech-us-gov-0227.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/musan/music/rfm/music-rfm-0137.wav</td>\n",
       "      <td>data/musan/speech/us-gov/speech-us-gov-0113.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/musan/music/jamendo/music-jamendo-0208.wav</td>\n",
       "      <td>data/musan/speech/librivox/speech-librivox-013...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               music  \\\n",
       "0  data/musan/music/fma-western-art/music-fma-wa-...   \n",
       "1            data/musan/music/rfm/music-rfm-0098.wav   \n",
       "2    data/musan/music/jamendo/music-jamendo-0035.wav   \n",
       "3            data/musan/music/rfm/music-rfm-0137.wav   \n",
       "4    data/musan/music/jamendo/music-jamendo-0208.wav   \n",
       "\n",
       "                                              speech  \n",
       "0  data/musan/speech/librivox/speech-librivox-012...  \n",
       "1  data/musan/speech/librivox/speech-librivox-007...  \n",
       "2    data/musan/speech/us-gov/speech-us-gov-0227.wav  \n",
       "3    data/musan/speech/us-gov/speech-us-gov-0113.wav  \n",
       "4  data/musan/speech/librivox/speech-librivox-013...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56232, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iteration Category data_type       Min       Max      Mean    Median  \\\n",
      "0             0   Speech  harmonic  -2912.31   1871.57     -0.12      1.06   \n",
      "1             0    Music  harmonic  -2074.94   2095.41     -0.24      0.86   \n",
      "2             0  Mixture  harmonic  -1527.35   1427.84     -0.11     17.55   \n",
      "3             0   Speech  skewness -11370.13 -11370.13 -11370.13 -11370.13   \n",
      "4             0    Music  skewness   -303.52   -303.52   -303.52   -303.52   \n",
      "...         ...      ...       ...       ...       ...       ...       ...   \n",
      "1681        280    Music  harmonic  -7150.13   7562.35     -0.14     12.76   \n",
      "1682        280  Mixture  harmonic  -2221.67   1596.61      0.06      4.53   \n",
      "1683        280   Speech  skewness  -4162.08  -4162.08  -4162.08  -4162.08   \n",
      "1684        280    Music  skewness    938.89    938.89    938.89    938.89   \n",
      "1685        280  Mixture  skewness -13005.03 -13005.03 -13005.03 -13005.03   \n",
      "\n",
      "          STD  \n",
      "0      470.51  \n",
      "1      515.07  \n",
      "2      314.11  \n",
      "3        0.00  \n",
      "4        0.00  \n",
      "...       ...  \n",
      "1681  1422.83  \n",
      "1682   351.16  \n",
      "1683     0.00  \n",
      "1684     0.00  \n",
      "1685     0.00  \n",
      "\n",
      "[1686 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from utils import load_audio,load_music,mix_signals\n",
    "from param import sampling_rate\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "df = pd.DataFrame(columns=['Iteration', 'Category','data_type', 'Min', 'Max', 'Mean', 'Median', 'STD'])\n",
    "list_data =[]\n",
    "for i in range(281):\n",
    "    # print(sampled_df.iloc[i][\"speech\"])\n",
    "    # print(sampled_df.iloc[i][\"music\"])\n",
    "\n",
    "    speech_wave = load_music(\"../\"+sampled_df.iloc[i][\"speech\"])\n",
    "    music_wave = load_music(\"../\"+sampled_df.iloc[i][\"music\"])\n",
    "    mixed_wave = mix_signals(\"../\"+sampled_df.iloc[i][\"speech\"],\"../\"+sampled_df.iloc[0][\"music\"])\n",
    "    #hpss_classification(speech_wave,music_wave,mixed_wave,sampling_rate)\n",
    "    harmonic_speech, percussive_speech = librosa.effects.hpss(speech_wave)\n",
    "    harmonic_music, percussive_music = librosa.effects.hpss(music_wave)\n",
    "    harmonic_mixture, percussive_mixture = librosa.effects.hpss(mixed_wave)\n",
    "    # plot_fig(harmonic_speech,percussive_speech,\"speech\")\n",
    "    # plot_fig(harmonic_music,percussive_music,\"music\")\n",
    "    # plot_fig(harmonic_mixture,percussive_mixture,\"mixture\")\n",
    "    for type_data in [\"harmonic\", \"skewness\"]:\n",
    "        if type_data == \"harmonic\":\n",
    "            data_speech = harmonic_speech\n",
    "            data_music = harmonic_music\n",
    "            data_mixture = harmonic_mixture\n",
    "        else:\n",
    "            data_speech = compute_skewness(harmonic_speech)\n",
    "            data_music = compute_skewness(harmonic_music)\n",
    "            data_mixture = compute_skewness(harmonic_mixture)\n",
    "\n",
    "    \n",
    "    # data_speech = np.abs(harmonic_speech- percussive_speech)\n",
    "    # data_music = np.abs(harmonic_music- percussive_music)\n",
    "    # data_mixture = np.abs(harmonic_mixture- percussive_mixture)\n",
    "    # # Append the data for each category to the DataFrame\n",
    "        list_data.append({\n",
    "            'Iteration': i,\n",
    "            'Category': 'Speech',\n",
    "            'data_type': type_data,\n",
    "            'Min': round(np.min(data_speech)*10000,2),\n",
    "            'Max': round(np.max(data_speech)*10000,2),\n",
    "            'Mean': round(np.mean(data_speech)*10000,2),\n",
    "            'Median': round(np.median(data_speech)*10000,2),\n",
    "            'STD': round(np.std(data_speech)*10000,2),\n",
    "        })\n",
    "        \n",
    "        list_data.append({\n",
    "            'Iteration': i,\n",
    "            'Category': 'Music',\n",
    "            'data_type': type_data,\n",
    "            'Min': round(np.min(data_music)*10000,2),\n",
    "            'Max': round(np.max(data_music)*10000,2),\n",
    "            'Mean': round(np.mean(data_music)*10000,2),\n",
    "            'Median': round(np.median(data_music)*10000,2),\n",
    "            'STD': round(np.std(data_music)*10000,2),\n",
    "        })\n",
    "        \n",
    "        list_data.append({\n",
    "            'Iteration': i,\n",
    "            'Category': 'Mixture',\n",
    "            'data_type': type_data,\n",
    "            'Min': round(np.min(data_mixture)*10000,2),\n",
    "            'Max': round(np.max(data_mixture)*10000,2),\n",
    "            'Mean': round(np.mean(data_mixture)*10000,2),\n",
    "            'Median': round(np.median(data_mixture)*10000,2),\n",
    "            'STD': round(np.std(data_mixture)*10000,2),\n",
    "        })\n",
    "\n",
    "# Print the DataFrame\n",
    "df = pd.DataFrame(list_data)\n",
    "print(df)\n",
    "\n",
    "\n",
    "    # speech_classification = classify_waveform(speech_wave, sampling_rate)\n",
    "    # music_classification = classify_waveform(music_wave, sampling_rate)\n",
    "    # mixed_classification = classify_waveform(mixed_wave, sampling_rate)\n",
    "    # print(f\"Ground Truth: Speech, Predicted: {speech_classification}\")\n",
    "    # print(f\"Ground Truth: Music, Predicted: {music_classification}\")\n",
    "    # print(f\"Ground Truth: Mixed, Predicted: {mixed_classification}\")\n",
    "    # print(\"------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Category', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hpss_classification_sk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iteration Category   data_type   Min   Max  Mean  Median   STD\n",
      "14          2  Mixture    harmonic -0.34  0.36   0.0     0.0  0.07\n",
      "26          4  Mixture    harmonic -0.31  0.31  -0.0     0.0  0.07\n",
      "23          3  Mixture  percussive -0.23  0.35   0.0     0.0  0.04\n",
      "20          3  Mixture    harmonic -0.33  0.37   0.0     0.0  0.07\n",
      "17          2  Mixture  percussive -0.41  0.43  -0.0     0.0  0.08\n",
      "11          1  Mixture  percussive -0.39  0.41   0.0     0.0  0.08\n",
      "8           1  Mixture    harmonic -0.29  0.24  -0.0     0.0  0.06\n",
      "29          4  Mixture  percussive -0.44  0.40  -0.0    -0.0  0.07\n",
      "5           0  Mixture  percussive -0.42  0.44   0.0     0.0  0.10\n",
      "2           0  Mixture    harmonic -0.27  0.27  -0.0    -0.0  0.06\n",
      "10          1    Music  percussive -0.01  0.01   0.0     0.0  0.00\n",
      "1           0    Music    harmonic -0.41  0.40  -0.0     0.0  0.08\n",
      "13          2    Music    harmonic -0.80  0.82   0.0    -0.0  0.25\n",
      "28          4    Music  percussive -0.49  0.49   0.0     0.0  0.07\n",
      "25          4    Music    harmonic -0.22  0.16   0.0     0.0  0.05\n",
      "16          2    Music  percussive -0.83  0.76   0.0    -0.0  0.13\n",
      "4           0    Music  percussive -0.82  0.90  -0.0    -0.0  0.15\n",
      "7           1    Music    harmonic -0.01  0.01   0.0     0.0  0.00\n",
      "19          3    Music    harmonic -0.37  0.32  -0.0    -0.0  0.10\n",
      "22          3    Music  percussive -0.21  0.19  -0.0    -0.0  0.02\n",
      "6           1   Speech    harmonic -0.27  0.22   0.0     0.0  0.04\n",
      "27          4   Speech  percussive -0.29  0.25  -0.0    -0.0  0.04\n",
      "24          4   Speech    harmonic -0.53  0.48  -0.0     0.0  0.09\n",
      "18          3   Speech    harmonic -0.42  0.44   0.0     0.0  0.07\n",
      "3           0   Speech  percussive -0.26  0.32   0.0    -0.0  0.04\n",
      "15          2   Speech  percussive -0.18  0.15   0.0     0.0  0.02\n",
      "12          2   Speech    harmonic -0.11  0.11   0.0    -0.0  0.01\n",
      "9           1   Speech  percussive -0.74  0.77  -0.0     0.0  0.07\n",
      "21          3   Speech  percussive -0.81  0.57   0.0     0.0  0.08\n",
      "0           0   Speech    harmonic -0.41  0.38   0.0    -0.0  0.06\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming harmonic_speech and percussive_speech are numpy arrays\n",
    "# representing the harmonic and percussive components of your speech signal\n",
    "def plot_fig(harmonic, persussive, class_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(harmonic)\n",
    "    plt.title(f'Harmonic Component of {class_name}')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(persussive)\n",
    "    plt.title(f'Percussive Component of {class_name}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.groupby(['Category','data_type']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_classification(signal_wave):\n",
    "    # Perform HPSS\n",
    "    harmonic, percussive = librosa.effects.hpss(signal_wave)\n",
    "\n",
    "    # Compute skewness vectors\n",
    "    skewness = compute_skewness(harmonic) + compute_skewness(percussive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('hpss_classification_grp_sk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Iteration Category data_type  avg_rskew_harmonic  avg_cskew_percussive\n",
      "0            0   Speech  harmonic          -277260.36             634157.98\n",
      "1            0    Music  harmonic                1.28                 -0.01\n",
      "2            0  Mixture  harmonic             8687.50               6220.83\n",
      "3            1   Speech  harmonic         -1509000.24            1286369.78\n",
      "4            1    Music  harmonic                3.51                  0.01\n",
      "..         ...      ...       ...                 ...                   ...\n",
      "838        279    Music  harmonic               -1.26                  0.13\n",
      "839        279  Mixture  harmonic             3845.79               7490.09\n",
      "840        280   Speech  harmonic         -1916664.73             996349.87\n",
      "841        280    Music  harmonic               -0.11                  0.09\n",
      "842        280  Mixture  harmonic            14969.00               6426.15\n",
      "\n",
      "[843 rows x 5 columns]\n",
      "The signal is classified as:\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy import stats\n",
    "\n",
    "def hpss_decomposition(signal, sr, lharm=17, lperc=17):\n",
    "    \"\"\"\n",
    "    Perform Harmonic-Percussive Source Separation (HPSS) on the given signal.\n",
    "    \"\"\"\n",
    "    S = librosa.stft(signal)\n",
    "    harmonic, percussive = librosa.decompose.hpss(S, margin=(1.0, 1.0), kernel_size=(lharm, lperc))\n",
    "    return harmonic, percussive\n",
    "\n",
    "def compute_skewness(matrix, axis):\n",
    "    \"\"\"\n",
    "    Compute the skewness of each row or column in the given matrix.\n",
    "    \"\"\"\n",
    "    return stats.skew(matrix, axis=axis)\n",
    "\n",
    "def classify_signal(harmonic, percussive, sr):\n",
    "    \"\"\"\n",
    "    Classify the signal into music, speech, or mixture based on thresholds.\n",
    "    \"\"\"\n",
    "    # Convert to Mel spectrograms\n",
    "    mel_harmonic = librosa.feature.melspectrogram(S=np.abs(harmonic), sr=sr)\n",
    "    mel_percussive = librosa.feature.melspectrogram(S=np.abs(percussive), sr=sr)\n",
    "\n",
    "    # Compute skewness\n",
    "    rskew_harmonic = compute_skewness(mel_harmonic, axis=1)\n",
    "    cskew_percussive = compute_skewness(mel_percussive, axis=0)\n",
    "\n",
    "    # Average skewness values\n",
    "    avg_rskew_harmonic = np.mean(rskew_harmonic)\n",
    "    avg_cskew_percussive = np.mean(cskew_percussive)\n",
    "\n",
    "    return avg_rskew_harmonic, avg_cskew_percussive\n",
    "    # # Classify based on thresholds\n",
    "    # if avg_rskew_harmonic > thresholds['harmonic'] and avg_cskew_percussive < thresholds['percussive']:\n",
    "    #     return 'Music'\n",
    "    # elif avg_rskew_harmonic < thresholds['harmonic'] and avg_cskew_percussive > thresholds['percussive']:\n",
    "    #     return 'Speech'\n",
    "    # else:\n",
    "    #     return 'Mixture'\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    df = pd.DataFrame(columns=['Iteration', 'Category','data_type', 'avg_rskew_harmonic', 'avg_cskew_percussive'])\n",
    "    list_data =[]\n",
    "    for i in range(281):\n",
    "        # print(sampled_df.iloc[i][\"speech\"])\n",
    "        # print(sampled_df.iloc[i][\"music\"])\n",
    "\n",
    "        speech_wave = load_music(\"../\"+sampled_df.iloc[i][\"speech\"])\n",
    "        music_wave = load_music(\"../\"+sampled_df.iloc[i][\"music\"])\n",
    "        mixed_wave = mix_signals(\"../\"+sampled_df.iloc[i][\"speech\"],\"../\"+sampled_df.iloc[0][\"music\"])\n",
    "        #hpss_classification(speech_wave,music_wave,mixed_wave,sampling_rate)\n",
    "         # Perform HPSS decomposition\n",
    "        avg_rskew_harmonic_speech, avg_cskew_percussive_speech = hpss_decomposition(speech_wave, 16000)\n",
    "        avg_rskew_harmonic_music, avg_cskew_percussive_music = hpss_decomposition(music_wave, 16000)\n",
    "        avg_rskew_harmonic_mixed, avg_cskew_percussive_mixed = hpss_decomposition(mixed_wave, 16000)\n",
    "\n",
    "\n",
    "        list_data.append({\n",
    "            'Iteration': i,\n",
    "            'Category': 'Speech',\n",
    "            'data_type': 'harmonic',\n",
    "            'avg_rskew_harmonic': round(np.min(avg_rskew_harmonic_speech).real*10000,2),\n",
    "            'avg_cskew_percussive': round(np.max(avg_cskew_percussive_speech).real*10000,2)\n",
    "        })\n",
    "        list_data.append({\n",
    "            'Iteration': i,\n",
    "            'Category': 'Music',\n",
    "            'data_type': 'harmonic',\n",
    "            'avg_rskew_harmonic': round(np.mean(avg_rskew_harmonic_music).real*10000,2),\n",
    "            'avg_cskew_percussive': round(np.median(avg_cskew_percussive_music).real*10000,2)\n",
    "        })\n",
    "        list_data.append({\n",
    "            'Iteration': i,\n",
    "            'Category': 'Mixture',\n",
    "            'data_type': 'harmonic',\n",
    "            'avg_rskew_harmonic': round(np.std(avg_rskew_harmonic_mixed).real*10000,2),\n",
    "            'avg_cskew_percussive': round(np.std(avg_cskew_percussive_mixed).real*10000,2)\n",
    "        })\n",
    "\n",
    "    # Print the DataFrame\n",
    "    df = pd.DataFrame(list_data)\n",
    "    print(df)\n",
    "    df.to_csv('newclassifcation.csv', index=False)\n",
    "    # Load an audio signal\n",
    "    signal, sr = librosa.load(librosa.ex('trumpet'))\n",
    "\n",
    "   \n",
    "    print(f'The signal is classified as:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_conditions(row):\n",
    "    # Classification conditions based on the provided criteria\n",
    "    if row['avg_rskew_harmonic'] < 0 and row['avg_cskew_percussive'] > 0:\n",
    "        return 'Speech'\n",
    "    elif abs(row['avg_cskew_percussive']) < 1:\n",
    "        return 'Music'\n",
    "    else:\n",
    "        return 'Mixture'  # Default to 'Mixture' if other conditions are not met\n",
    "\n",
    "# Apply the classification function to the DataFrame\n",
    "df['Classified_Category'] = df.apply(classify_conditions, axis=1)\n",
    "\n",
    "# Show the updated DataFrame with the classified categories\n",
    "df[['Iteration', 'Category', 'avg_rskew_harmonic', 'avg_cskew_percussive', 'Classified_Category']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
